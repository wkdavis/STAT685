---
title: "modeling"
author: 
  - William K Davis III
date: "`r Sys.Date()`"
output:
  pdf_document:
    citation_package: natbib
    extra_dependencies: ["amsmath", "float"]
bibliography: citations.bib
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache=TRUE,
  fig.pos = "!H"
  )
library(readr)
library(dplyr)
library(tsibble)
library(fabletools)
library(feasts)
library(ggplot2)
library(lubridate)
library(caret)
library(gridExtra)
library(corrplot)
library(tidyr)
library(GGally)
library(fable)
library(fable.prophet)
library(scales)
```

```{r data-prep}
bikets <- readr::read_csv("SeoulBikeData.csv",
                        col_names = c("Date",
                                      "BikeCount",
                                      "Hour",
                                      "Temperature",
                                      "Humidity",
                                      "WindSpeed",
                                      "Visibility",
                                      "Dewpoint",
                                      "SolarRadiation",
                                      "Rainfall",
                                      "Snowfall",
                                      "Seasons",
                                      "Holiday",
                                      "FunctionalDay"),
                        skip = 1,
                        col_types = cols("Hour" = col_time(format = "%H"),
                                         Seasons = "f",
                                         Holiday = "f",
                                         FunctionalDay = "f")) %>%
  mutate(
    Hour = parse_date_time(
      paste(Date, Hour),
      orders = c("dmy HMS", "dmY HMS"),
      tz = "Asia/Seoul"
    ),
    .before = everything(),
    Date = NULL,
    BikeCount = ifelse(FunctionalDay == "Yes", BikeCount, NA),
    FunctionalDay = NULL,
    Holiday = forcats::fct_relevel(Holiday, "No Holiday"),
    Workday = factor(ifelse(((lubridate::wday(Hour, label = TRUE) %in% c("Sat", "Sun")) | Holiday == "Holiday"), "Not Workday", "Workday"), levels = c("Workday", "Not Workday"))
  ) %>%
  as_tsibble(index = Hour)

bikets_cv <- filter_index(bikets, . ~ "2018-10-11 23:00:00 KST")


bikets_cv_stretched <- stretch_tsibble(slice_head(bikets_cv, n = -24),
                                   .step = 24,
                                   .init = nrow(bikets_cv)*.8,
                                   .id = "fold")

bikets_test_stretched <- stretch_tsibble(bikets,
                                        .step = 24,
                                   .init = nrow(bikets_cv),
                                   .id = "fold")
```

# Modeling

## Time-Series Modeling

Figure 1 highlights the heteroskedasticity in the bike demand.
This can be corrected using a simple Box-Cox transformation (@boxcox).
If we let $x_t$ represent the value of raw Bike Count data and $y_t$ represent
the transformed data, we have

\begin{equation}
\label{eq:bc}
y_t=\frac{x_t^{\lambda}-1}{\lambda}
\end{equation}

```{r bc}
l <- bikets_cv %>%
  features(BikeCount, features = guerrero) %>%
  pull(lambda_guerrero)
```

where $\lambda=$ `r l` was selected using Guerrero's method (@guerrero).

### SARIMAX

Traditional linear regression models can be adapted to handle the autocovariance
structure of time series by assuming that the errors follow a seasonal ARIMA 
(SARIMA) process
instead of the traditional $\epsilon\sim\text{iid }N(0,\sigma^2)$. This results
in the modified regression equation

\begin{equation}
\label{eq:ols}
y_t=\beta_0+\sum_{j=1}^k\beta_jz_{jt}+x_t
\end{equation}

where:

* $y_t$ is the response variable at time $t$
* $\beta_0$ is the traditional intercept
* $z_{1t},\dots,z_{kt}$ are the $k$ exogenous regressors observed at
time $t$.
* $\beta_1,\dots,\beta_k$ are the regression coefficients.
* $x_t$ are the regression errors, which are assumed to follow an SARIMA process
as in (\ref{eq:sarima}).

(\ref{eq:ols}) is often referred to as SARIMAX for
Seasonal ARIMA with eXogenous regressors.

\begin{equation}
\label{eq:sarima}
\Phi_P(B^S)\phi(B)\nabla_S^D\nabla^dx_t=\Theta_Q(B^S)\theta(B)w_t
\end{equation}

where:

* $\Phi_P(B^S)$ are the $P$ seasonal autoregressive components
* $\phi(B)$ are the $p$ autoregressive components
* $\nabla_S^D$ are the $D$ seasonal differences
* $\nabla^d$ are the $d$ differences
* $\Theta_Q(B^S)$ are the $Q$ seasonal moving average components
* $\theta(B)$ are the $q$ moving average components
* $w_t\sim\text{ iid }N(0,\sigma_w^2)$ is the traditional Gaussian white noise

(@shumway). SARIMA models of this form are often written
$ARIMA(p,d,q)\times(P,D,Q)_S$. A KPSS test indicates that the data is
non-stationary and requires $D=1$ seasonal difference (@kpss).
Autocorrelation and partial
autocorrelation plots can be used to determine the other ARIMA parameters (AR
and MA).

```{r acf, fig.cap="Correlograms for $\\nabla_Sy_t$", fig.height=2}
# bikets_cv %>%
#   mutate(BikeCount = difference(box_cox(BikeCount, l), lag = 24)) %>%
#   gg_tsdisplay(BikeCount, plot_type = "partial", lag_max = 24*7) +
#   labs(title = "Transformed Bike Count")

grid.arrange(
bikets_cv %>%
  mutate(BikeCount = difference(box_cox(BikeCount, l), lag = 24)) %>%
  ACF(BikeCount, lag_max = 24*7) %>%
  autoplot() + labs(title = expression(paste("ACF of ", nabla[S]*y[t]))) + theme(axis.title.y = element_blank(), axis.text.x = element_text(size = 6)),
bikets_cv %>%
  mutate(BikeCount = difference(box_cox(BikeCount, l), lag = 24)) %>%
  PACF(BikeCount, lag_max = 24*7) %>%
  autoplot()+ labs(title = expression(paste("PACF of ", nabla[S]*y[t]))) + theme(axis.title.y = element_blank(), axis.text.x = element_text(size = 6)),
ncol = 2)
```

Figure 6 indicates a seasonal autoregressive model with $P=6$ and $p\ge12$.
This is a large number of seasonal autoregressive terms and will almost
certainly result in characteristic roots inside the unit circle, causing
the model to be unstable. A quick search of the
parameter space using the `R` function `fable::ARIMA` 
(@fable) to automatically select values for
$p, d, q, Q$ results in no stable $ARIMA(p,d,q)\times(6,1,Q)_{24}$ models
being found.

When the data
exhibit higher frequency or multiple types of seasonality (such as daily and
weekly, in our case), an alternative solution for modeling seasonality
can be to use fourier terms (@fpp3). 
Introducing the fourier terms will result in a model of the form


\begin{align}
\label{eq:arimafourier}
y_t&=\beta_0+\sum_{j=1}^k\beta_jz_{jt}+s_d(t,m)+s_w(t,n)+x_t\\
\label{eq:fourierday}
s_d(t,m)&=\sum_{i=1}^m\left[\alpha_i\sin\left(\frac{2\pi it}{24}\right)+\beta_i\cos\left(\frac{2\pi it}{24}\right)\right]\\
\label{eq:fourierweek}
s_w(t,n)&=\sum_{l=1}^n\left[\gamma_l\sin\left(\frac{2\pi lt}{168}\right)+\delta_l\cos\left(\frac{2\pi lt}{168}\right)\right]
\end{align}

where:

* $\beta_0+\sum_{j=1}^k\beta_jz_{jt}$ are the intercept, independent variables
and their coefficients, as defined in (\ref{eq:ols})
* $s_d(t,m)$ are the $m$ daily seasonality fourier terms
* $s_w(t,n)$ are the $n$ weekly seasonality fourier terms
* $x_t$ are the model errors, which are assumed to follow a $ARIMA(p,d,q)$
process (note that in contrast to (\ref{eq:ols}) this is a non-seasonal ARIMA
model; the seasonality is omitted from the ARIMA process because it is captured
by the fourier terms).

```{r arima-fourier, fig.height=4, fig.cap="Residuals for an ARIMAX(2,1,2) model with daily & weekly fourier terms"}
allfits <- readRDS("allfits.RDS")
bike_fourier <- allfits[,"ARIMAfourier"]
# bike_fourier <- model(bikets_cv, ARIMA(box_cox(BikeCount, l) ~ pdq(2,1,2) + PDQ(0,0,0) + fourier(24, 10) + fourier(24 * 7, 40) + Dewpoint + Humidity + Rainfall + SolarRadiation + Temperature + Visibility + WindSpeed + Seasons + Workday))
bike_fourier %>%
  residuals() %>%
  gg_tsdisplay(.resid, plot_type = "partial", lag_max = 24*7) +
  labs(title = "ARIMAX(2,1,2) with daily & weekly fourier terms")
```

The fourier model resulted in the selection of an $ARIMA(2,1,2)$ process for the
regression residuals. The residual ACF and PACF plots from the fourier model
(Figure 7) still show significant autocorrelation.
Autocorrelation in the residuals violates the Gauss-Markov assumptions, meaning
the model should not be used for inference. The goal of this analysis is to
generate accurate (point) forecasts, which are still valid even when the errors
exhibit autocorrelation.

Finally, we will fit a linear regression with SARIMA errors to the data where
all parameters
$p,d,q,P,D,Q$ are chosen automatically using `fable::ARIMA`, which relies on
the algorithm developed by (@autoarima). This resulted in the selection of an
$SARIMA(1,0,1)\times(5,1,0)_{24}$ for $x_t$.

```{r auto-sarima, fig.cap="Residuals for an $SARIMAX(1,0,1)\\times(5,1,0)_{24}$"}
allfits <- readRDS("allfits.RDS")
bike_autoarima <- allfits[,"ARIMAauto"]
# bike_autoarima <- model(bikets_cv, ARIMA(box_cox(BikeCount, l) ~ pdq(1,0,1) + PDQ(5,1,0) + Dewpoint + Humidity + Rainfall + SolarRadiation + Temperature + Visibility + WindSpeed + Seasons + Workday))
bike_autoarima %>%
  residuals() %>%
  gg_tsdisplay(.resid, plot_type = "partial", lag_max = 24*7)+
  labs(title = expression(paste("SARIMAX(1,0,1)"%*%"(5,1,0)"[24])))
```

Figure 8 shows that, while the autocorrelation (and partial autocorrelation)
has been reduced over the $ARIMAX(2,1,2)$ with fourier terms, there is still
repeating statistically significant autocorrelation. It is possible that the
seasonal patterns in the data are too complex or too high frequency to be
captured by an ARIMA, ARIMAX, or SARIMAX model. Therefore, we will explore more
modern time series techniques.

### Prophet

Prophet uses a generalized additive model (GAM) to capture the different
features of the time series (@prophet). Our model will take the form

\begin{equation}
\label{eq:prophet}
y(t)=g(t)+s_d(t,m)+s_w(t,n)+\sum_{j=1}^k\beta_jz_{jt}+\epsilon_t
\end{equation}

where:

* $g(t)$ is a piecewise constant function to represent the trend in the series
* $s_d(t,m)$ as in (\ref{eq:fourierday})
* $s_w(t,n)$ as in (\ref{eq:fourierweek})
* $\sum_{j=1}^k\beta_jz_{jt}$ is as in \ref{eq:ols}
* $\epsilon_t$ is the model error

One major benefit of (\ref{eq:prophet}) is that it is much faster to fit than
a model with ARIMA terms. The downside is that it does not explicitly capture
(non-seasonal) $AR(p)$ and $MA(q)$ terms. We will use
`fable.prophet::prophet` to train the model in `R` (@fableprophet). The `R`
function sets a number of desirable default parameter values as
recommended by (@prophet). In order to properly tune the model to our dataset
we will use TSCV (see [Model Selection and Test Error](#Model-Selection-and-Test-Error)) to select $m$ and $n$. The TSCV will
consist of 63 folds/time-slices. We will search over the parameter space
$m\in\{1, 2, 5, 10, 20, 50\}$ and $n\in\{3, 5, 10, 20, 50\}$, giving us
$6\times5=30$ possible parameter combinations.

```{r prophet-cv, fig.cap="Cross-validated MAE for Prophet models with various numbers of (daily and weekly) fourier terms", fig.height=3, fig.width=3}
structure(list(day = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 
2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 
5L, 5L, 6L, 6L, 6L, 6L, 6L), .Label = c("1", "2", "5", "10", 
"20", "50"), class = "factor"), week = structure(c(1L, 2L, 3L, 
4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 
5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L), .Label = c("3", 
"5", "10", "20", "50"), class = "factor"), MAE = c(365.105299071547, 
364.95245347683, 363.393678198711, 307.339502718768, 273.406754695837, 
321.085658194055, 320.760611535959, 319.981972651726, 307.429216334244, 
273.486947095138, 304.917905827215, 305.364486550836, 301.154567694556, 
285.250471917666, 273.239637157694, 302.328145809333, 303.182360025435, 
298.950654734633, 280.10231949376, 272.182229877828, 302.95634912311, 
302.755944321119, 299.570668869166, 279.378451608946, 271.857750727527, 
302.308493806848, 302.394497389872, 298.797209802857, 279.496729378455, 
271.424610246633)), row.names = c(NA, -30L), class = c("tbl_df", 
"tbl", "data.frame")) %>%
  ggplot(aes(x = day, y = week, label = round(MAE), fill = MAE)) +
  geom_bin_2d() +
  geom_text(size = 2.5) +
  scale_fill_gradient(low = "pink", high = muted("red")) +
  theme_minimal() +
  labs(title = "Prophet out-of-sample MAE",
       x = "m: Number of 'day' fourier terms",
       y = "n: Number of 'week' fourier terms")
```

Figure 9 identifies $m=50$ and $n=50$ as the optimal number of fourier terms
(they minimize MAE).
We fit this model to the entire training dataset and analyze the residuals.

```{r prophet-resid, fig.cap="Residuals for a Prophet model with $m=50$ (daily) and $n=50$ (weekly) fourier terms", fig.height=2}
# prophetBasic <- bikets_cv %>%
#   mutate(BikeCount = na_if(BikeCount, 0),
#          Workday = factor(gsub(" ", "", as.character(Workday)), levels = c("Workday", "NotWorkday"))) %>%
#         model(
#           prophet = prophet(
#             box_cox(BikeCount, l) ~ season(
#               period = "day",
#               type = "additive",
#               order = 50,
#               name = "day"
#             ) + season(
#               period = "week",
#               type = "additive",
#               order = 50,
#               name = "week"
#             ) + growth() + Workday + Dewpoint + Humidity + Rainfall + SolarRadiation + Temperature + Visibility + WindSpeed + Seasons
#           )
#         )
# saveRDS(prophetBasic, "prophetBasic.RDS")

prophetBasic <- readRDS("prophetBasic.RDS")
# prophetBasic %>%
#   residuals() %>%
#   gg_tsdisplay(y = .resid, plot_type = "partial", lag_max = 24*7) +
#   labs(title = "Residuals from Prophet model with m=50, n=50")

grid.arrange(
prophetBasic %>%
  residuals() %>%
  ACF(.resid, lag_max = 24*7) %>%
  autoplot() + 
  labs(title = "ACF of Prophet model",
       subtitle = "m=50 daily, n=50 weekly fourier terms") +
  theme(axis.title.y = element_blank(), 
        axis.text.x = element_text(size = 6),
        title = element_text(size = 8)),
prophetBasic %>%
  residuals() %>%
  PACF(.resid, lag_max = 24*7) %>%
  autoplot() + 
  labs(title = "PACF of Prophet model",
       subtitle = "m=50 daily, n=50 weekly fourier terms") +
  theme(axis.title.y = element_blank(), 
        axis.text.x = element_text(size = 6),
        title = element_text(size = 8)),
ncol = 2)
```

As can be seen from the PACF of the residuals in Figure 10,
there is still autocorrelation not captured by the model. Prophet does not offer
native support for $AR(p)$ terms, but we can add them manually. Based on the
PACF it looks like $AR(12)$ and $AR(1)_{24}$ terms would be appropriate for
this model.

```{r prophet-AR-acf, fig.cap="Residuals for Prophet model with $AR(12)(1)_{24}$, $m=50$ (daily), $n=50$ (weekly) fourier terms", fig.height=2}
# prophetAR24 <- bikets_cv %>%
#   mutate(BikeCount = na_if(BikeCount, 0),
#          Workday = factor(gsub(" ", "", as.character(Workday)), levels = c("Workday", "NotWorkday")),
#          BikeCountL1 = lag(BikeCount, 1),
#          BikeCountL2 = lag(BikeCount, 2),
#          BikeCountL3 = lag(BikeCount, 3),
#          BikeCountL4 = lag(BikeCount, 4),
#          BikeCountL5 = lag(BikeCount, 5),
#          BikeCountL6 = lag(BikeCount, 6),
#          BikeCountL7 = lag(BikeCount, 7),
#          BikeCountL8 = lag(BikeCount, 8),
#          BikeCountL9 = lag(BikeCount, 9),
#          BikeCountL10 = lag(BikeCount, 10),
#          BikeCountL11 = lag(BikeCount, 11),
#          BikeCountL12 = lag(BikeCount, 12),
#          BikeCountL24 = lag(BikeCount, 24)) %>%
#   tidyr::fill(starts_with("BikeCountL"), .direction = "downup") %>%
#   slice(n=-(1:24)) %>%
#   model(prophet12 = prophet(
#             box_cox(BikeCount, l) ~ season(
#               period = "day",
#               type = "additive",
#               order = 50,
#               name = "day"
#             ) + season(
#               period = "week",
#               type = "additive",
#               order = 50,
#               name = "week"
#             ) + growth() + Workday + Dewpoint + Humidity + Rainfall + SolarRadiation + Temperature + Visibility + WindSpeed + Seasons + BikeCountL1 + BikeCountL2 + BikeCountL3 + BikeCountL4 + BikeCountL5 + BikeCountL6 + BikeCountL7 + BikeCountL8 + BikeCountL9 + BikeCountL10 + BikeCountL11 + BikeCountL12 + BikeCountL24
#           ))
# saveRDS(prophetAR24, "prophetAR24.RDS")

prophetAR24 <- readRDS("prophetAR24.RDS")
# prophetAR24 %>%
#   residuals() %>%
#   gg_tsdisplay(y = .resid, plot_type = "partial", lag_max = 24*7) +
#   labs(title = expression(paste("Residuals from Prophet with ", AR(12),",",AR(1)[24],", m=50, n=50")))

grid.arrange(
prophetAR24 %>%
  residuals() %>%
  ACF(.resid, lag_max = 24*7) %>%
  autoplot() + 
  labs(title = "ACF from Prophet",
       subtitle = expression(paste("with ", AR(12)(1)[24],", m=50, n=50"))) +
  theme(axis.title.y = element_blank(), 
        axis.text.x = element_text(size = 6),
        title = element_text(size = 8)),
prophetAR24 %>%
  residuals() %>%
  PACF(.resid, lag_max = 24*7) %>%
  autoplot() + 
  labs(title = "PACF from Prophet",
       subtitle = expression(paste("with ", AR(12)(1)[24],", m=50, n=50"))) +
  theme(axis.title.y = element_blank(), 
        axis.text.x = element_text(size = 6),
        title = element_text(size = 8)),
ncol = 2)
```

While the autoregression present in the PACF (Figure 11) is certainly improved, it
is still present in a statistically significant manner. However, given that our
goal is forecasting/prediction rather than inference, it should not pose a
problem for the analysis. The addition of the autoregressive terms to the
Prophet model has resulted in a decrease in out-of-sample MAE from 271 to
193.

### fasster

One interesting feature of the residual PACFs from both the SARIMAX
model (Figure 8) and the Prophet model (Figure 11) is that
the seasonal "spikes" are parabolic in shape. That is, the PACF starts high
at lag 24, then decreases until reaching a min around lag 72 or
lag 96 before rising again to another peak at lag 168 (which is
the one week lag). This shape might be why the AR and fourier terms are not
capturing all of the autocorrelation in the data: the strength of the
correlation appears to be a non-linear function of time.

One reason for this
shape in the PACF plot could be the presence of switching seasonality. More
specifically, it's possible that the seasonality (not just the series itself)
is different on working days and non-working days. A
hand-wavy analysis would say that, given there are seven days in a
week, any single day is, on "average", $7/2=3.5$ days away from any other day.
For weekdays specifically, they are 3.5 days away from a weekend (again, on
average).

Therefore, the trough in the PACF curve occurring between 3 and 4 days
might be due to that lag most frequently correlating weekdays with weekend days,
resulting in lower correlation than when weekdays are compared with other
weekdays, which would occurr most frequently at lags $<3$ and $>4$. This is
certainly not a technical explanation of the pattern, but it is plausible. The
previously employed techniques (SARIMAX and Prophet) don't allow for changing
seasonality. The fasster methodology allows for the seasonal (and trend)
component(s) of the model to be switched based on the "state" of a given
observation, which is defined via a discrete variable passed to the model
(@fasster). In
our case, this variable would be *Workday*, which identifies the observation
as a workday or non-workday (weeeknd or holiday). Different seasonality
coefficients can then be fit based on whether a bike count is observed on
a workday or non-workday. This is an improvement over the previous models,
which simply include the *Workday* variable as a regression coefficient,
reducing its effect to a shift in the level of the series.


---
title: "LSTM Python"
author: "Pei-Yin Yang"
date: "`r Sys.Date()`"
output: html_document
---

## Deep Learning - LSTM for Bike Demand Prediction

```{r python setup, include=FALSE}
library(reticulate)
use_python("/Users/peiyin/opt/anaconda3/bin/python3") #Select the version of Python to be used by reticulate.
```

References:

[Multivariate Time Series Forecasting with LSTMs in Keras](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/)

[Convert a Time Series to a Supervised Learning Problem in Python](https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/)

[Tune LSTM Hyperparameters with Keras for Time Series Forecasting](https://machinelearningmastery.com/tune-lstm-hyperparameters-keras-time-series-forecasting/)

[Use Learning Curves to Diagnose Machine Learning Model Performance](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)

[Difference Between a Batch and an Epoch in a Neural Network](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)

[Scale Data for Long Short-Term Memory Networks in Python](https://machinelearningmastery.com/how-to-scale-data-for-long-short-term-memory-networks-in-python/)

```{python, include = FALSE}
# Let's import all packages that we may need:

import sys 
import numpy as np # linear algebra
from numpy import concatenate
from scipy.stats import randint
from datetime import datetime
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
from matplotlib import pyplot # this is used for the plot the graph 
import seaborn as sns # used for plot interactive graph. 
from sklearn.model_selection import train_test_split # to split the data into two parts
from sklearn.model_selection import KFold # use for cross validation
from sklearn.preprocessing import StandardScaler # for normalization
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline # pipeline making
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics # for the check the error and accuracy of the model
from sklearn.metrics import mean_squared_error,r2_score
import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

## for Deep-learing:
import keras
from keras.layers import Dense
from keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import SGD 
from keras.callbacks import EarlyStopping
from keras.utils import np_utils
import itertools
from keras.layers import LSTM
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import Dropout
```

### Basic Data Preparation

```{python}
# load data
def parse(x):
    return datetime.strptime(x, '%d/%m/%Y %H')
bike = pd.read_csv("SeoulBikeData.csv", encoding ="unicode_escape",
                 parse_dates = [['Date', 'Hour']], 
                 index_col = 0, date_parser = parse)
# manually specify column names
bike.columns = ["BikeCount",
                "Temperature", "Humidity",
                "WindSpeed", "Visibility",
                "Dewpoint", "SolarRadiation",
                "Rainfall", "Snowfall",
                "Seasons", "Holiday", "FunctionalDay"]
bike.index.name = "Datetime"
# bike = bike.drop('FunctionalDay', 1) #maybe we can drop FunctionalDay
print(bike.head())
# save to file
bike.to_csv('SeoulBikeData2.csv')
# dimension of the dataset
bike.shape
```

## Multivariate LSTM Forecast Model

### LSTM Data Preparation:

We will frame the dataset as a supervised learning problem, meaning that we will predict the bike demand at current hour(t) or desired hour given the weather conditions and bike supply at the prior time step.

Use the following *series_to_supervised()* function to transform the dataset:

```{python}
# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    # forecast sequence (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
    # put it all together
    agg = concat(cols, axis=1)
    agg.columns = names
    # drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg
```

```{python}
# prepare data for lstm
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder

# load dataset
dataset = read_csv('SeoulBikeData2.csv', header=0, index_col=0)
values = dataset.values
```

### **1 hour input times steps**

We can see there are 12 input variables (input series) and 1 output variable (bike demand at the current hour).

```{python}
# convert categorical features to numeric encoding
# integer encode direction
encoder = LabelEncoder()
values[:,9] = encoder.fit_transform(values[:,9])
values[:,10] = encoder.fit_transform(values[:,10])
values[:,11] = encoder.fit_transform(values[:,11])
# ensure all data is float
values = values.astype('float32')
# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)
# frame as supervised learning
reframed = series_to_supervised(scaled, 1, 1)
# drop columns we don't want to predict
reframed.drop(reframed.columns[[13,14,15,16,17,18,19,20,21,22,23]], axis=1, inplace=True)
print(reframed.head())
print(reframed.shape)
```

### Define and Fit Model(one-hour input time step)

```{python}
# split into train and test sets
values = reframed.values
N = len(dataset.index)
days_for_cv = 50
n_train_hours = (N - days_for_cv* 24 )
#n_train_hours = int(N * 0.7)
train = values[:n_train_hours, :]
test = values[n_train_hours:, :]
# split into input and outputs
train_X, train_y = train[:, :-1], train[:, -1]
test_X, test_y = test[:, :-1], test[:, -1]
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)
```

**Model Architecture**

1) LSTM with 50 neurons in the first visible layer
2) 1 neuron in the output layer for predicting Bike Demand.
3) The input shape will be 1 time step with 12 features.
4) I use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.
5) The model will be fit for 60 training epochs with a batch size of 72.

There more extensions can be tried out.

Dropout, layers, regularization, optimization algorithm, etc.

```{python, echo = FALSE}
# design network
model = Sequential()
model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
#model.add(Dropout(0.2))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')
# fit network
history = model.fit(train_X, train_y, epochs=60, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)
# summarize history for loss
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.title('model loss')
pyplot.ylabel('loss')
pyplot.xlabel('epoch')
pyplot.legend()
pyplot.show()
```

```{python, echo = FALSE}
# make a prediction
yhat = model.predict(test_X)
test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
# invert scaling for forecast
inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:,0]
# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]

# calculate RMSE
rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))
print('Test RMSE: %.3f' % rmse)
# calculate MAE
# import the module
from sklearn.metrics import mean_absolute_error as mae
mae = mae(inv_y, inv_yhat)
print('Test mean absolute error : %.3f' % mae)
```

```{python, echo = FALSE}
## time steps, every step is one hour (you can easily convert the time step to the actual time index)
## for a demonstration purpose, I only compare the predictions in 24*7 hours. 
## Generate Dates for index
aa=[x for x in range(24)]
pyplot.plot(aa, inv_y[:24], marker='.', label="actual")
pyplot.plot(aa, inv_yhat[:24], 'r', label="prediction")
pyplot.ylabel('Bike Demand', size=10)
pyplot.xlabel('Time step', size=10)
pyplot.legend(fontsize=10)
pyplot.show()
```

### **12 hour input times steps**

The changes needed to train the model on multiple previous time steps are quite minimal.

We can see there are 12 x 24 input variables (input series) and 1 x 12 output variable (bike demand at the current hour).

```{python, echo = FALSE}
# specify the number of lag hours
n_hours = 12 # use 12 hours of data as input
n_features = 12

# load dataset
dataset = read_csv('SeoulBikeData2.csv', header=0, index_col=0)
values = dataset.values
# integer encode direction
encoder = LabelEncoder()
values[:,9] = encoder.fit_transform(values[:,9])
values[:,10] = encoder.fit_transform(values[:,10])
values[:,11] = encoder.fit_transform(values[:,11])
# ensure all data is float
values = values.astype('float32')
# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)
# frame as supervised learning
reframed = series_to_supervised(scaled, n_hours, 1)
# drop columns we don't want to predict
#reframed.drop(reframed.columns[[289,290,291,292,293,294,295,296,297,298,299]], axis=1, inplace=True)
print(scaled.shape)
print(reframed.head())
print(reframed.shape)
```

### Define and Fit Model (`py n_hours`-hour input time step)

```{python}
# split into train and test sets
values = reframed.values
N = len(dataset.index)
days_for_cv = 50
n_train_hours = (N - days_for_cv* 24 ) 
#n_train_hours = int(N * 0.7)
train = values[:n_train_hours, :]
test = values[n_train_hours:, :]
# split into input and outputs
n_obs = n_hours * n_features
train_X, train_y = train[:, :n_obs], train[:, -n_features]
test_X, test_y = test[:, :n_obs], test[:, -n_features]
print(train_X.shape, len(train_X), train_y.shape)
```

Next, we can reshape our input data correctly to reflect the time steps and features.

```{python}
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))
test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)
```

**Model Architecture**

1) LSTM with 50 neurons in the first visible layer
2) 1 neuron in the output layer for predicting Bike Demand.
3) The input shape will be 12 time step with 12 features.
4) I use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.
5) The model will be fit for 60 training epochs with a batch size of 72.

```{python, echo = FALSE}
# design network
model = Sequential()
model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')
# fit network
history = model.fit(train_X, train_y, epochs=60, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)
# plot history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()
```

The learnig cure looks good. "A good fit is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values."

```{python, echo = FALSE}
# make a prediction
yhat = model.predict(test_X)
test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))
# invert scaling for forecast
inv_yhat = concatenate((yhat, test_X[:, -11:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:,0]
# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = concatenate((test_y, test_X[:, -11:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]
# calculate RMSE
rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))
print('Test RMSE: %.3f' % rmse)
# calculate MAE
# import the module
from sklearn.metrics import mean_absolute_error as mae
mae = mae(inv_y, inv_yhat)
print('Test mean absolute error : %.3f' % mae)
```

```{python, echo = FALSE}
## time steps, every step is one hour (you can easily convert the time step to the actual time index)
## for a demonstration purpose, I only compare the predictions in 24 hours. 

aa=[x for x in range(24)]
pyplot.plot(aa, inv_y[:24], marker='.', label="actual")
pyplot.plot(aa, inv_yhat[:24], 'r', label="prediction")
pyplot.ylabel('Bike Demand', size=10)
pyplot.xlabel('Time step', size=10)
pyplot.legend(fontsize=15)
pyplot.show()
```

### **24 hour input times steps**

We can see there are 24 x 24 input variables (input series) and 1 x 12 output variable (bike demand at the current hour).

```{python, echo = FALSE}
# specify the number of lag hours
n_hours = 24 # use 12 hours of data as input
n_features = 12

# load dataset
dataset = read_csv('SeoulBikeData2.csv', header=0, index_col=0)
values = dataset.values
# integer encode direction
encoder = LabelEncoder()
values[:,9] = encoder.fit_transform(values[:,9])
values[:,10] = encoder.fit_transform(values[:,10])
values[:,11] = encoder.fit_transform(values[:,11])
# ensure all data is float
values = values.astype('float32')
# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)
# frame as supervised learning
reframed = series_to_supervised(scaled, n_hours, 1)
# drop columns we don't want to predict
#reframed.drop(reframed.columns[[289,290,291,292,293,294,295,296,297,298,299]], axis=1, inplace=True)
print(scaled.shape)
print(reframed.head())
print(reframed.shape)
```

### Define and Fit Model (`py n_hours`-hour input time step)

```{python}
# split into train and test sets
values = reframed.values
N = len(dataset.index)
days_for_cv = 50
n_train_hours = (N - days_for_cv* 24 ) 
#n_train_hours = int(N * 0.7)
train = values[:n_train_hours, :]
test = values[n_train_hours:, :]
# split into input and outputs
n_obs = n_hours * n_features
train_X, train_y = train[:, :n_obs], train[:, -n_features]
test_X, test_y = test[:, :n_obs], test[:, -n_features]
print(train_X.shape, len(train_X), train_y.shape)
```

```{python}
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))
test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)
```

**Model Architecture**

1) LSTM with 50 neurons in the first visible layer
2) 1 neuron in the output layer for predicting Bike Demand.
3) The input shape will be 24 time step with 12 features.
4) I use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.
5) The model will be fit for 60 training epochs with a batch size of 72.

```{python, echo = FALSE}
# design network
model = Sequential()
model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')
# fit network
history = model.fit(train_X, train_y, epochs=60, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)
# plot history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()
```

```{python, echo = FALSE}
# make a prediction
yhat = model.predict(test_X)
test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))
# invert scaling for forecast
inv_yhat = concatenate((yhat, test_X[:, -11:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:,0]
# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = concatenate((test_y, test_X[:, -11:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]
# calculate RMSE
rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))
print('Test RMSE: %.3f' % rmse)
# calculate MAE
# import the module
from sklearn.metrics import mean_absolute_error as mae
mae = mae(inv_y, inv_yhat)
print('Test mean absolute error : %.3f' % mae)
```

```{python, echo = FALSE}
## time steps, every step is one hour (you can easily convert the time step to the actual time index)
## for a demonstration purpose, I only compare the predictions in 24 hours. 

aa=[x for x in range(24)]
pyplot.plot(aa, inv_y[:24], marker='.', label="actual")
pyplot.plot(aa, inv_yhat[:24], 'r', label="prediction")
pyplot.ylabel('Bike Demand', size=10)
pyplot.xlabel('Time step', size=10)
pyplot.legend(fontsize=15)
pyplot.show()
```


Some to-do:

* One-hot encoding categorical variables.

```{python}
#one hot encoding on multiple categorical columns
# categorical_cols = ['Seasons', 'Holiday', 'FunctionalDay'] 
# dataset = pd.get_dummies(dataset, columns = categorical_cols)
# dataset = dataset.drop(['Seasons_Winter','Holiday_No Holiday','FunctionalDay_No'], axis = 1) #drop redundant columns
# print(dataset.head())
# print(dataset.shape)
```


* Making all series stationary with differencing and seasonal adjustment.


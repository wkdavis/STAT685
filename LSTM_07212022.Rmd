---
title: "LSTM_07212022"
author: "Pei-Yin Yang, Max W. Kutschinski"
date: "7/21/2022"
output: html_document
---

### Intro to Long Short-Term Memory (LSTM) Recurrent Neural Network

LSTM is a variety of RNN. The benefit of the Long Short-Term Memory (LSTM) network over other recurrent networks comes from an improved method of back-propagating the error. Hochreiter and Schmidhuber called it "constant error back propagation" [@hochreiter_1997_long].

The Constant Error Carousel (CEC) is the magic of the LSTM in that it prevents vanishing gradients. It's denoted as follows:

$c_{t+1}=c_{t}$ ∗ forget gate + new input ∗ input gate

In the case of regular RNNs during backpropagation, the derivative of an activation function, such as a logistic, will be less than one. Therefore over time, the repeated multiplication of that value against the weights will lead to a vanishing gradient.

In the case of an LSTM, we only multiply the cell state by a forget gate, which acts as both the weights and the activation function for the cell state. As long as that forget gate equals one, the information from the previous cell state passes through unchanged. This is why LSTM can deal with more intricate problems than the RNN by keeping a constant flow of error throughout the backpropagation from cell to cell.

### LSTM forecasting Bike Rental demand

### Methods

#### Data processing

**Normalization**

It is a good idea for machine learning algorithms that fit a model that uses a weighted sum of input variables, such as linear regression, logistic regression, and artificial neural networks (deep learning) to normalize the data[@brownlee_2020_how]. Here, by using the scikit-learn object *MinMaxScaler*, the normalization scales each input variable separately to the range [0,1], which is the range for floating-point values where we have the most precision. A value is normalized as follows:

* y = (x-min) / (max-min) 

```{r python setup, include=FALSE}
library(reticulate)
use_python("/Users/peiyin/opt/anaconda3/bin/python3") #Select the version of Python to be used by reticulate.
```

**Use the following *series_to_supervised()* function to transform the dataset**

```{python}
# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    # forecast sequence (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
    # put it all together
    agg = concat(cols, axis=1)
    agg.columns = names
    # drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg
```

#### Network Training and Forecasting

A built-in library - Keras in Python, developed by Google, is used for model building. Each input data was a list of lagged hours of bike rental count and the output data for that particular input was the bike demand for the next hour. One-hour, 12-hour, and 24-hour lagged timestep are used for the input data. The processed dataset was partitioned into train and test set where about 86% of the total data was used for model training and the remaining 14% was used for testing. Table 1. shows the implementation details.

: Table 1. Details of the LSTM model setup.

| Items | Detail |
|:------|:-----|
| Prediction Target | Bike rental demand forecasting in the city of Seoul for the next 24 hours |
| Input Variable | Observed daily bike rental count and weather condition data during 12/01/2017 - 11/30/2018.<br/> Lagged time step: 1hr, 12hr, 24hr |
| Training Parameters | - Learning Rate: 0.001<br/>- Number of nodes: 50<br/>- Number of epochs:60<br/>- Batch size: 24<br/>- Dropout: 0.2|

**Definition of Hyper-parameters**

- nodes(neurons): units accepting a vector of real-valued inputs and producing a single real-valued output.

- batch size: how many obs. are used at each step, converge faster with a larger value.

- epoch: an iteration over all training obs.

- dropout: regularization method to prevent over-fitting

- learning rate: define how quickly the network updates its parameters

**Hyper-parameter tuning is on the to-do list**

#### Model Evaluation

Forecast performance was assessed using Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and the learning curve. A good fit is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values.

#### Results and Discussions

```{python, include = FALSE}
# Let's import all packages that we may need:

import sys 
import numpy as np # linear algebra
from numpy import concatenate
from scipy.stats import randint
from datetime import datetime
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
from matplotlib import pyplot # this is used for the plot the graph 
import seaborn as sns # used for plot interactive graph. 
from sklearn.model_selection import train_test_split # to split the data into two parts
from sklearn.model_selection import KFold # use for cross validation
from sklearn.preprocessing import StandardScaler # for normalization
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline # pipeline making
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics # for the check the error and accuracy of the model
from sklearn.metrics import mean_squared_error,r2_score
import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

## for Deep-learning:
import keras
from keras.layers import Dense
from keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import SGD
from tensorflow.keras import activations
from keras.callbacks import EarlyStopping
from keras.utils import np_utils
import itertools
from keras.layers import LSTM
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import Dropout
```

```{python Basic Data Preparation, include = FALSE}
# load data
# date-time parsing function for loading the dataset
def parse(x):
    return datetime.strptime(x, '%d/%m/%Y %H')
bike = pd.read_csv("SeoulBikeData.csv", encoding ="unicode_escape",
                 parse_dates = [['Date', 'Hour']], 
                 index_col = 0, date_parser = parse)
# manually specify column names
bike.columns = ["BikeCount",
                "Temperature", "Humidity",
                "WindSpeed", "Visibility",
                "Dewpoint", "SolarRadiation",
                "Rainfall", "Snowfall",
                "Seasons", "Holiday", "FunctionalDay"]
bike.index.name = "Datetime"
bike = bike.drop(['FunctionalDay','Dewpoint'], axis = 1) #maybe we can drop FunctionalDay
print(bike.head())
# save to file
bike.to_csv('SeoulBikeData3.csv')
```

```{python, include = FALSE}
# prepare data for lstm
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder
```

#### **1 hour lagged time steps as input**

```{python, include = FALSE}
# load dataset
dataset = read_csv('SeoulBikeData3.csv', header=0, index_col=0)
#one hot encoding on multiple categorical columns
categorical_cols = ['Seasons', 'Holiday'] 
dataset = pd.get_dummies(dataset, columns = categorical_cols)
values = dataset.values
#### integer encode direction##### convert categorical features to numeric encoding
#encoder = LabelEncoder()
#values[:,8] = encoder.fit_transform(values[:,8])
#values[:,9] = encoder.fit_transform(values[:,9])
#################################
# ensure all data is float
values = values.astype('float32')
# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)
# frame as supervised learning
reframed = series_to_supervised(scaled, 1, 1)
# drop columns we don't want to predict
reframed.drop(reframed.columns[[15,16,17,18,19,20,21,22,23,24,25,26,27]], axis=1, inplace=True)
print(reframed.head())
print(reframed.shape)
```

```{python, include = FALSE}
# split into train and test sets
values = reframed.values
N = len(dataset.index)
days_for_cv = 50
n_train_hours = (N - days_for_cv* 24 )
#n_train_hours = int(N * 0.7)
train = values[:n_train_hours, :]
test = values[n_train_hours:, :]
# split into input and outputs
train_X, train_y = train[:, :-1], train[:, -1]
test_X, test_y = test[:, :-1], test[:, -1]
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)
```

#### **1 hour lagged time steps as input - Model Architecture**

1) LSTM with 64 neurons in the first visible layer

2) LSTM with 32 neurons in the second visible layer

3) Dropout 50%

4) 1 neuron in the output layer for predicting Bike Rental Demand.

5) The input shape will be 1 time step with 14 features.

6) I use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.

7) The model will be fit for 80 training epochs with a batch size of 50.

There more extensions can be tried out.

Layers, regularization, optimization algorithm, etc.

```{python, include = FALSE}
# design network
model = Sequential()
model.add(LSTM(64, activation = "relu",return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(LSTM(32, activation = "relu",input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dropout(0.5))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')
# fit network
history = model.fit(train_X, train_y, epochs=80, batch_size=50, 
validation_data=(test_X, test_y), verbose=2, shuffle=False)
```

```{python fig.cap='Figure 1. Model loss - 1 hour lagged time steps as input', echo = FALSE}
# summarize history for loss
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.title('model loss')
pyplot.ylabel('MAE')
pyplot.xlabel('Number of Epochs')
pyplot.legend()
pyplot.show()
```

Although from figure 1. we can see both errors converge fairly fast, the proposed network is not a good fit since there is a gap between the training error and test error.

```{python, echo = FALSE}
# make a prediction
yhat = model.predict(test_X)
test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
# invert scaling for forecast
inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:,0]
# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]

# calculate RMSE
rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))
print('Test RMSE: %.3f' % rmse)
# calculate MAE
# import the module
from sklearn.metrics import mean_absolute_error as mae
mae = mae(inv_y, inv_yhat)
print('Test mean absolute error : %.3f' % mae)
```

```{python fig.cap='Figure 2. Forcasted Bike Rental Demand of one day - 1 hour lagged time steps as input', echo = FALSE}
## time steps, every step is one hour (you can easily convert the time step to the actual time index)
## for a demonstration purpose, I only compare the predictions in 24*7 hours. 
## Generate Dates for index
aa=[x for x in range(24)]
pyplot.plot(aa, inv_y[:24], marker='.', label="actual")
pyplot.plot(aa, inv_yhat[:24], 'r', label="prediction")
pyplot.ylabel('Bike Rental Demand', size=10)
pyplot.xlabel('Hours', size=10)
pyplot.legend(fontsize=10)
pyplot.show()
```

#### **12 hour lagged time steps as input**

```{python, include = FALSE}
# specify the number of lag hours
n_hours = 12 # use 12 hours of data as input
n_features = 14

# load dataset
dataset = read_csv('SeoulBikeData3.csv', header=0, index_col=0)
#one hot encoding on multiple categorical columns
categorical_cols = ['Seasons', 'Holiday'] 
dataset = pd.get_dummies(dataset, columns = categorical_cols)
values = dataset.values
#### integer encode direction####
#encoder = LabelEncoder()
#values[:,8] = encoder.fit_transform(values[:,8])
#values[:,9] = encoder.fit_transform(values[:,9])
#################################
# ensure all data is float
values = values.astype('float32')
# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)
# frame as supervised learning
reframed = series_to_supervised(scaled, n_hours, 1)
# drop columns we don't want to predict
#reframed.drop(reframed.columns[[289,290,291,292,293,294,295,296,297,298,299]], axis=1, inplace=True)
print(scaled.shape)
print(reframed.head())
print(reframed.shape)
```

```{python, include = FALSE}
# split into train and test sets
values = reframed.values
N = len(dataset.index)
days_for_cv = 50
n_train_hours = (N - days_for_cv* 24 ) 
#n_train_hours = int(N * 0.7)
train = values[:n_train_hours, :]
test = values[n_train_hours:, :]
# split into input and outputs
n_obs = n_hours * n_features
train_X, train_y = train[:, :n_obs], train[:, -n_features]
test_X, test_y = test[:, :n_obs], test[:, -n_features]
print(train_X.shape, len(train_X), train_y.shape)
```

```{python, include = FALSE}
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))
test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)
```

#### **12 hour lagged time steps as input - Model Architecture**

1) LSTM with 64 neurons in the first visible layer

2) LSTM with 32 neurons in the second visible layer

3) Dropout 50%

4) 1 neuron in the output layer for predicting Bike Rental Demand.

5) The input shape will be 12 time step with 14 features.

6) I use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.

7) The model will be fit for 80 training epochs with a batch size of 50.

```{python, include = FALSE}
# design network
model = Sequential()
model.add(LSTM(64, activation = "relu",return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(LSTM(32, activation = "relu",input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dropout(0.5))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')
# fit network
history = model.fit(train_X, train_y, epochs=80, batch_size=50, validation_data=(test_X, test_y), verbose=2, shuffle=False)
```

```{python fig.cap='Figure 3. Model loss - 12 hour lagged time steps as input', echo = FALSE}
# plot history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.title('model loss')
pyplot.ylabel('MAE')
pyplot.xlabel('Number of Epochs')
pyplot.legend()
pyplot.show()
```

The Loss vs Epoch curve is shown in the Fig. 3 in which the progress of model while training is represented. Both training and testing loss decreases in a smooth fashion fairly quickly.  

```{python, echo = FALSE}
# make a prediction
yhat = model.predict(test_X)
test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))
# invert scaling for forecast
inv_yhat = concatenate((yhat, test_X[:, -13:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:,0]
# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = concatenate((test_y, test_X[:, -13:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]
# calculate RMSE
rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))
print('Test RMSE: %.3f' % rmse)
# calculate MAE
# import the module
from sklearn.metrics import mean_absolute_error as mae
mae = mae(inv_y, inv_yhat)
print('Test mean absolute error : %.3f' % mae)
```

```{python fig.cap='Figure 4. Forcasted Bike Rental Demand of one day - 12 hour lagged time steps as input', echo = FALSE}
## time steps, every step is one hour (you can easily convert the time step to the actual time index)
## for a demonstration purpose, I only compare the predictions in 24 hours. 

aa=[x for x in range(24)]
pyplot.plot(aa, inv_y[:24], marker='.', label="actual")
pyplot.plot(aa, inv_yhat[:24], 'r', label="prediction")
pyplot.ylabel('Bike Demand', size=10)
pyplot.xlabel('Hours', size=10)
pyplot.legend(fontsize=15)
pyplot.show()
```

#### **24 hour lagged time steps as input**

```{python, echo = FALSE}
# specify the number of lag hours
n_hours = 24 # use 24 hours of data as input
n_features = 14

# load dataset
dataset = read_csv('SeoulBikeData3.csv', header=0, index_col=0)
#one hot encoding on multiple categorical columns
categorical_cols = ['Seasons', 'Holiday'] 
dataset = pd.get_dummies(dataset, columns = categorical_cols)
values = dataset.values
#### integer encode direction##### convert categorical features to numeric encoding
#encoder = LabelEncoder()
#values[:,8] = encoder.fit_transform(values[:,8])
#values[:,9] = encoder.fit_transform(values[:,9])
#################################
# ensure all data is float
values = values.astype('float32')
# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)
# frame as supervised learning
reframed = series_to_supervised(scaled, n_hours, 1)
# drop columns we don't want to predict
#reframed.drop(reframed.columns[[289,290,291,292,293,294,295,296,297,298,299]], axis=1, inplace=True)
print(scaled.shape)
print(reframed.head())
print(reframed.shape)
```

```{python}
# split into train and test sets
values = reframed.values
N = len(dataset.index)
days_for_cv = 50
n_train_hours = (N - days_for_cv* 24 ) 
#n_train_hours = int(N * 0.7)
train = values[:n_train_hours, :]
test = values[n_train_hours:, :]
# split into input and outputs
n_obs = n_hours * n_features
train_X, train_y = train[:, :n_obs], train[:, -n_features]
test_X, test_y = test[:, :n_obs], test[:, -n_features]
print(train_X.shape, len(train_X), train_y.shape)
```

```{python}
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))
test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)
```

#### **24 hour lagged time steps as input - Model Architecture**

1) LSTM with 64 neurons in the first visible layer

2) LSTM with 32 neurons in the second visible layer

3) Dropout 50%

4) 1 neuron in the output layer for predicting Bike Rental Demand.

5) The input shape will be 24 time step with 14 features.

6) I use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.

7) The model will be fit for 80 training epochs with a batch size of 50.

```{python, include = FALSE}
# design network
model = Sequential()
model.add(LSTM(64, activation = "relu",return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(LSTM(32, activation = "relu",input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dropout(0.5))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')
# fit network
history = model.fit(train_X, train_y, epochs=80, batch_size=50, validation_data=(test_X, test_y), verbose=2, shuffle=False)
```

```{python fig.cap='Figure 5. Model loss - 24 hour lagged time steps as input', echo = FALSE}
# plot history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.title('model loss')
pyplot.ylabel('MAE')
pyplot.xlabel('Number of Epochs')
pyplot.legend()
pyplot.show()
```

```{python, echo = FALSE}
# make a prediction
yhat = model.predict(test_X)
test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))
# invert scaling for forecast
inv_yhat = concatenate((yhat, test_X[:, -13:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:,0]
# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = concatenate((test_y, test_X[:, -13:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]
# calculate RMSE
rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))
print('Test RMSE: %.3f' % rmse)
# calculate MAE
# import the module
from sklearn.metrics import mean_absolute_error as mae
mae = mae(inv_y, inv_yhat)
print('Test mean absolute error : %.3f' % mae)
```

```{python fig.cap='Figure 6. Forcasted Bike Rental Demand of one day - 24 hour lagged time steps as input', echo = FALSE}
## time steps, every step is one hour (you can easily convert the time step to the actual time index)
## for a demonstration purpose, I only compare the predictions in 24 hours. 

aa=[x for x in range(24)]
pyplot.plot(aa, inv_y[:24], marker='.', label="actual")
pyplot.plot(aa, inv_yhat[:24], 'r', label="prediction")
pyplot.ylabel('Bike Demand Rental', size=10)
pyplot.xlabel('Hours', size=10)
pyplot.legend(fontsize=15)
pyplot.show()
```

### References

